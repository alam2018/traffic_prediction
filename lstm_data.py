# -*- coding: utf-8 -*-
"""lstm_data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jT4CBPpVClaxyh7k0vuPlyuI8FSO_Qr_
"""

#from google.colab import drive
#drive.mount('/content/drive')

#Define if the simulation is for uRLLC or eMBB slice
web_traffic = True
m2m = False
m2m_model1 = False

#define train size
result_size = 60
web_train_size = 7000
web_test_size = 7000

m2m_train_size = 2000
m2m_test_size = 2000

web_memory_size = 60
m2m_memory_size = 15

# Part 1 - Data Preprocessing

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
#from google.colab import files
import pandas as pd

# Importing the training set

if web_traffic == True:
    eMBB = True
    if eMBB == True:
#        dataset_train = pd.read_csv('/home/mdaa/traffic_prediction/dataSet/web_traffic/train/traffic_simulation.csv', sep=';')
        dataset_train = pd.read_csv('/home/mdaa/traffic_prediction/dataSet/web_traffic2/train/traffic_simulation_100usr.csv', sep=';')
    else:
        dataset_train = pd.read_csv('/content/drive/My Drive/Colab Notebooks/traffic_prediction/data_URLLC/traffic_simulation.csv', sep=';')
        
if m2m == True:
    if m2m_model1 == True:
        dataset_train = pd.read_csv('/home/mdaa/traffic_prediction/dataSet/m2m_traffic/model_1/traffic_simulation_3000_1800usr.csv', sep=';')
    else:
        dataset_train = pd.read_csv('/home/mdaa/traffic_prediction/dataSet/m2m_traffic/model_2/traffic_simulation_3000_1800usr.csv', sep=';')

    
#Insert a new column containing the 'byte' value converted to 'Mb' for training
dataset_train['TotalData_Mb'] = dataset_train.iloc[:, 2].values / 1000000

#training_set = dataset_train.iloc[:, 1:2].values
training_set = dataset_train.iloc[:, 4].values
training_set = training_set.reshape(-1,1)

# Feature Scaling
from sklearn.preprocessing import MinMaxScaler
sc = MinMaxScaler(feature_range = (0, 1))
training_set_scaled = sc.fit_transform(training_set)

# Creating a data structure with 60 timesteps and 1 output
X_train = []
y_train = []

if web_traffic == True:
    for i in range(60, web_train_size):
        X_train.append(training_set_scaled[i-60:i, 0])
        y_train.append(training_set_scaled[i, 0])
    X_train, y_train = np.array(X_train), np.array(y_train)
    
if m2m == True:
    for i in range(m2m_memory_size, m2m_train_size):
        X_train.append(training_set_scaled[i-m2m_memory_size:i, 0])
        y_train.append(training_set_scaled[i, 0])
    X_train, y_train = np.array(X_train), np.array(y_train)

# Reshaping
X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))

# Part 2 - Building the RNN

# Importing the Keras libraries and packages
from keras.models import Sequential
from keras.layers import Activation, Dense
from keras.layers import LSTM
from keras.layers import Dropout
from keras.optimizers import adam, SGD, RMSprop, Nadam
from keras.callbacks import EarlyStopping, ModelCheckpoint

# Initialising the RNN
regressor = Sequential()

# Adding the first LSTM layer and some Dropout regularisation
regressor.add(LSTM(units = 100, return_sequences = True, input_shape = (X_train.shape[1], 1)))

# Adding a second LSTM layer and some Dropout regularisation
regressor.add(LSTM(units = 100, return_sequences = True))
#regressor.add(LSTM(units = 50))
regressor.add(Dropout(0.5))


# Adding a third LSTM layer and some Dropout regularisation
regressor.add(LSTM(units = 100, return_sequences = True))
#regressor.add(LSTM(units = 50))
regressor.add(Dropout(0.5))


# Adding a fourth LSTM layer and some Dropout regularisation
regressor.add(LSTM(units = 100, return_sequences = True))
#regressor.add(LSTM(units = 100))
regressor.add(Dropout(0.5))


# Adding a fifth LSTM layer and some Dropout regularisation
#regressor.add(LSTM(units = 100))
#regressor.add(Dropout(0.5))

# Adding a fifth LSTM layer and some Dropout regularisation
#regressor.add(LSTM(units = 100, return_sequences = True))
#regressor.add(Dropout(0.5))


#delete fter experiment

# Adding a sixth LSTM layer and some Dropout regularisation
#regressor.add(LSTM(units = 100, return_sequences = True))
#regressor.add(Dropout(0.5))


# Adding a seventh LSTM layer and some Dropout regularisation
#regressor.add(LSTM(units = 100, return_sequences = True))
#regressor.add(Dropout(0.5))

# Adding a eighth LSTM layer and some Dropout regularisation
regressor.add(LSTM(units = 100))
regressor.add(Dropout(0.5))



# Adding the output layer
regressor.add(Dense(units = 1))
#regressor.add(Activation('softmax'))

# Compiling the RNN
#opt = SGD(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.01, amsgrad=False)
#opt= SGD(lr=0.01, momentum=0.01, decay=0.01, nesterov=False)
#opt= Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)
#opt = adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.00001, amsgrad=False)
#opt = SGD
opt = adam
regressor.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics=['accuracy'])
#regressor.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics=['mean_squared_error'])


# Create callbacks
import datetime as datetime
import tensorflow as tf
import csv

class MyCustomCallback(tf.keras.callbacks.Callback):

  def on_train_batch_begin(self, batch, logs=None):
    #print('Training: batch {} begins at {}'.format(batch, datetime.datetime.now().time()))
    print('Training: batch {} begins at {}'.format(batch, datetime.datetime.now().time()))
    

  def on_train_batch_end(self, batch, logs=None):
    print('Training: batch {} ends at {}'.format(batch, datetime.datetime.now().time()))

  def on_test_batch_begin(self, batch, logs=None):
    print('Evaluating: batch {} begins at {}'.format(batch, datetime.datetime.now().time()))

  def on_test_batch_end(self, batch, logs=None):
    print('Evaluating: batch {} ends at {}'.format(batch, datetime.datetime.now().time()))
    
  def create_cpu_time_report (self, batch, logs=None):
    with open('lstm_data_cpu.csv', 'w', newline='') as file:
        writer = csv.writer(file)
        writer.writerows()




# Fitting the RNN to the Training set
#model_data = regressor.fit(X_train, y_train, validation_split = 0.30, epochs = 2000,  callbacks=[MyCustomCallback()], batch_size = 32,  verbose=1)
if web_traffic == True:
    model_data = regressor.fit(X_train, y_train, validation_split = 0.30, epochs = 500, batch_size = 32,  verbose=1)
if m2m == True:
    model_data = regressor.fit(X_train, y_train, validation_split = 0.30, epochs = 500, batch_size = 32,  verbose=1)

# list all data in history
#print(model_data.history.keys())

# summarize history for accuracy
#plt.plot(model_data.history['mean_squared_error'])
#plt.plot(model_data.history['val_mean_squared_error'])
#plt.plot(model_data.history['acc'])
#plt.plot(model_data.history['val_acc'])
#plt.title('model accuracy')
#plt.ylabel('accuracy')
#plt.xlabel('epoch')
#plt.legend(['train', 'test'], loc='upper right')
#plt.savefig('model_accuracy.png',bbox_inches="tight", pad_inches=0, dpi=300)
#plt.show()

# summarize history for loss
plt.plot(model_data.history['loss'])
plt.plot(model_data.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.savefig('model_loss_data.png',bbox_inches="tight", pad_inches=0, dpi=300)
plt.show()

# Part 3 - Making the predictions and visualising the results

# Getting the test dataset

if web_traffic == True:
    if eMBB == True:
#        dataset_test = pd.read_csv('/home/mdaa/traffic_prediction/dataSet/web_traffic/test/traffic_simulation.csv', sep=';')
         dataset_test = pd.read_csv('/home/mdaa/traffic_prediction/dataSet/web_traffic2/test/traffic_simulation_100usr.csv', sep=';')
    else:
        dataset_test = pd.read_csv('/content/drive/My Drive/Colab Notebooks/traffic_prediction/data_URLLC/traffic_simulation_testData.csv', sep=';')
        
if m2m == True:
#    m2m_model1 = False
    if m2m_model1 == True:
        dataset_test = pd.read_csv('/home/mdaa/traffic_prediction/dataSet/m2m_traffic/model1_test/traffic_simulation_3000_1800usr.csv', sep=';')
    else:
        dataset_test = pd.read_csv('/home/mdaa/traffic_prediction/dataSet/m2m_traffic/model2_test/traffic_simulation_3000_1800usr.csv', sep=';')

#Insert a new column containing the 'byte' value converted to 'Mb' for testing
dataset_test['TotalData_Mb'] = dataset_test.iloc[:, 2].values / 1000000
#real_stock_price = dataset_test.iloc[60:90, 1:2].values
if web_traffic == True:
    real_test_data = dataset_test.iloc[web_test_size:web_test_size + result_size, 4].values
if m2m == True:
    real_test_data = dataset_test.iloc[m2m_test_size:m2m_test_size + result_size, 4].values
#real_test_data = dataset_test.iloc[web_test_size:web_test_size + result_size, 4].values

# Getting the predicted stock price of 2017
#dataset_total = pd.concat((dataset_train['Open'], dataset_test['Open']), axis = 0)
#inputs = dataset_total[len(dataset_total) - len(dataset_test) - 60:].values
#inputs = dataset_test.iloc[:, 1:2].values
inputs = dataset_test.iloc[:, 4].values

inputs = inputs.reshape(-1,1)
inputs = sc.transform(inputs)
X_test = []
inputs_size = len(inputs)

if web_traffic == True:
    for i in range(web_test_size, web_test_size + result_size):
#       X_test.append(inputs[i-60:i, 0])
        X_test.append(inputs[i-60:i, 0])
        
if m2m == True:
    for i in range(1775, 1835):
#       X_test.append(inputs[i-60:i, 0])
        X_test.append(inputs[i-m2m_memory_size:i, 0])
X_test = np.array(X_test)
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))
#X_test = np.reshape(X_test, -1)

predicted_test_traffic = regressor.predict(X_test)

predicted_test_traffic = sc.inverse_transform(predicted_test_traffic)

# Visualising the results
init_val = 0;
prediction_size = 60
#x_val = []
#for loop in range (prediction_size):
#    x_val.append(init_val)
#    init_val += 0.1
    

if web_traffic == True:
    plt.plot(real_test_data, color = 'red', label = 'Test Traffic (different dataset)')
    plt.plot(predicted_test_traffic, color = 'blue', label = 'Predicted traffic')
    plt.ylim (43,50)
    plt.title('Traffic Prediction')
    plt.xlabel('Time Index (s)')
    plt.ylabel('Data (MBps)')
    plt.legend(loc='best', prop={'size': 6})
elif m2m == True:
    plt.plot(real_test_data, color = 'red', label = 'Test Traffic (different dataset)')
    plt.plot(predicted_test_traffic, color = 'blue', label = 'Predicted traffic')
    plt.ylim (0,6)
    plt.title('Traffic Prediction')
    plt.xlabel('Time Index (s)')
    plt.ylabel('Data (MBps)')
    plt.legend(loc='best', prop={'size': 6})

plt.savefig('lstm_testData_data.png',bbox_inches="tight", pad_inches=0, dpi=300)

plt.show()

#Generating output for validation dataset
# Creating a data structure with 60 timesteps and 1 output

#real_valid_data = dataset_train.iloc[6000:6060, 4].values
if web_traffic == True:
    real_valid_data = dataset_train.iloc[web_train_size : web_train_size + result_size, 4].values
if m2m == True:
    real_valid_data = dataset_train.iloc[m2m_train_size:m2m_train_size+result_size, 4].values

X_valid = []
if web_traffic == True:
    for i in range(web_train_size,web_train_size + result_size):
#    X_valid.append(training_set_scaled[i-60:i, 0])
        X_valid.append(training_set_scaled[i-60:i, 0])
        
if m2m == True:
    for i in range(m2m_train_size,m2m_train_size+result_size):
#    X_valid.append(training_set_scaled[i-60:i, 0])
        X_valid.append(training_set_scaled[i-m2m_memory_size:i, 0])
X_valid = np.array(X_valid)
X_valid = np.reshape(X_valid, (X_valid.shape[0], X_valid.shape[1], 1))

predicted_packet_valid = regressor.predict(X_valid)
predicted_packet_valid = sc.inverse_transform(predicted_packet_valid)

# Visualising the validation data
#from google.colab import files
#test = plt.figure()


if web_traffic == True:
    plt.plot(real_valid_data, color = 'green', label = 'Test Traffic (same dataset)')
    plt.plot(predicted_packet_valid, color = 'orange', label = 'Predicted traffic')
    plt.ylim (43,50)
    plt.title('Traffic Prediction')
    plt.xlabel('Time Index (s)')
    plt.ylabel('Data (MBps)')
    plt.legend(loc='best', prop={'size': 6})
elif m2m == True:
    plt.plot(x_val, real_valid_data, color = 'green', label = 'Test Traffic (same dataset)')
    plt.plot(x_val, predicted_packet_valid, color = 'orange', label = 'Predicted traffic')
    plt.ylim (0,6)
    plt.title('Traffic Prediction')
    plt.xlabel('Time Index (s)')
    plt.ylabel('Data (MBps)')
    plt.legend(loc='best', prop={'size': 6})

#plt.draw()
#fig = plt.figure(figsize=(9, 11))
plt.savefig('lstm_valData_data.png',bbox_inches="tight", pad_inches=0, dpi=300)
plt.show()
#files.download('test.pdf')

# Visualising the combination

plt.plot(real_test_data, color = 'red', label = 'Test Traffic (different dataset)')
plt.plot(predicted_test_traffic, color = 'blue', label = 'Predicted traffic')
plt.plot(real_valid_data, color = 'green', label = 'Test Traffic (same dataset)')
plt.plot(predicted_packet_valid, color = 'orange', label = 'Predicted traffic')
if web_traffic == True:
    plt.ylim (43,50)
elif m2m == True:
    plt.ylim (0,6)
plt.title('Traffic Prediction')
plt.xlabel('Time Index (s)')
plt.ylabel('Data (MBps)')
plt.legend(loc='best', prop={'size': 6})

#plt.draw()
#fig = plt.figure(figsize=(9, 11))
plt.savefig('lstm_comb_data.png',bbox_inches="tight", pad_inches=0, dpi=300)
plt.show()
#files.download('test.pdf')

from sklearn.metrics import mean_squared_error

from math import sqrt

mse = mean_squared_error(real_test_data, predicted_test_traffic)

rmse_test = sqrt(mse)

print('RMSE test data: %f' % rmse_test)

mse = mean_squared_error(real_valid_data, predicted_packet_valid)

rmse_val = sqrt(mse)

print('RMSE val data: %f' % rmse_val)

#Write the test and predicted data
predicted_test_traffic = np.reshape (predicted_test_traffic, (np.product(predicted_test_traffic.shape),))

df = pd.DataFrame(list(zip(real_test_data, predicted_test_traffic)),
              columns=['Test Data (Mb)','Predicted Data (Mb)'])

df.to_csv("lstm_data_test.csv", index = None, header=True)

#write the validation and predicted data
predicted_packet_valid = np.reshape (predicted_packet_valid, (np.product(predicted_packet_valid.shape),))

write_val = pd.DataFrame(list(zip(real_valid_data, predicted_packet_valid)),
              columns=['Validation Data (Mb)','Predicted Data (Mb)'])
write_val.to_csv("lstm_data_val.csv", index = None, header=True)

#model_data.history = np.reshape (model_data.history, (np.product(model_data.history.shape),))

#write_model = pd.DataFrame (model_data.history, index = None)

#print (model_data.data)