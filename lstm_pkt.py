# -*- coding: utf-8 -*-
"""lstm_pkt.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JFeIoJmHxpR3MtYarH4qFyKcvzivIMwm
"""

#from google.colab import drive
#drive.mount('/content/drive')

#Define the simulation
web_traffic = False
m2m = True
m2m_model1 = True

#define train size
result_size = 60
web_train_size = 7000
web_test_size = 7000

m2m_train_size = 60000
m2m_test_size = 1775



# Part 1 - Data Preprocessing

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
#from google.colab import files
import pandas as pd

# Importing the training set
if web_traffic == True:
    eMBB = True
    if eMBB == True:
        dataset_train = pd.read_csv('/home/mdaa/traffic_prediction/web_traffic/train/traffic_simulation.csv', sep=';')
    else:
        dataset_train = pd.read_csv('/content/drive/My Drive/Colab Notebooks/traffic_prediction/data_URLLC/train_data/traffic_simulation.csv', sep=';')
  
if m2m == True:
#    m2m_model1 = False
    if m2m_model1 == True:
        dataset_train = pd.read_csv('/home/mdaa/traffic_prediction/m2m_traffic/model_1/traffic_simulation.csv', sep=';')
    else:
        dataset_train = pd.read_csv('/home/mdaa/traffic_prediction/m2m_traffic/model_2/traffic_simulation.csv', sep=';')
        
#training_set = dataset_train.iloc[:, 1:2].values
training_set = dataset_train.iloc[:, 1:2].values

# Feature Scaling
from sklearn.preprocessing import MinMaxScaler
sc = MinMaxScaler(feature_range = (0, 1))
training_set_scaled = sc.fit_transform(training_set)

# Creating a data structure with 60 timesteps and 1 output
X_train = []
y_train = []
if web_traffic == True:
    for i in range(60, web_train_size):
        X_train.append(training_set_scaled[i-60:i, 0])
        y_train.append(training_set_scaled[i, 0])
    X_train, y_train = np.array(X_train), np.array(y_train)
    
if m2m == True:
    for i in range(30, 10000):
        X_train.append(training_set_scaled[i-30:i, 0])
        y_train.append(training_set_scaled[i, 0])
    X_train, y_train = np.array(X_train), np.array(y_train)

# Reshaping
X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))

# Part 2 - Building the RNN

# Importing the Keras libraries and packages
from keras.models import Sequential
from keras.layers import Activation, Dense
from keras.layers import LSTM
from keras.layers import Dropout
from keras.optimizers import adam, SGD, RMSprop, Nadam

# Initialising the RNN
regressor = Sequential()

# Adding the first LSTM layer and some Dropout regularisation
regressor.add(LSTM(units = 100, return_sequences = True, input_shape = (X_train.shape[1], 1)))

# Adding a second LSTM layer and some Dropout regularisation
regressor.add(LSTM(units = 100, return_sequences = True))
#regressor.add(LSTM(units = 50))
regressor.add(Dropout(0.5))


# Adding a third LSTM layer and some Dropout regularisation
regressor.add(LSTM(units = 100, return_sequences = True))
#regressor.add(LSTM(units = 100))
regressor.add(Dropout(0.5))


# Adding a fourth LSTM layer and some Dropout regularisation
regressor.add(LSTM(units = 100, return_sequences = True))
#regressor.add(LSTM(units = 100))
regressor.add(Dropout(0.5))


# Adding a fifth LSTM layer and some Dropout regularisation
regressor.add(LSTM(units = 100))
regressor.add(Dropout(0.5))


# Adding the output layer
regressor.add(Dense(units = 1))
#regressor.add(Activation('softmax'))

# Compiling the RNN
#opt = SGD(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.01, amsgrad=False)
#opt= SGD(lr=0.01, momentum=0.01, decay=0.01, nesterov=False)
#opt= Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)
#opt = adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.00001, amsgrad=False)
opt = adam
regressor.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics=['accuracy'])
#regressor.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics=['mean_squared_error'])

# Fitting the RNN to the Training set
if web_traffic == True:
    model_data = regressor.fit(X_train, y_train, validation_split = 0.30, epochs = 500, batch_size = 32,  verbose=1)
if m2m == True:
    model_data = regressor.fit(X_train, y_train, validation_split = 0.30, epochs = 500, batch_size = 32,  verbose=1)

# list all data in history
#print(model_data.history.keys())

# summarize history for accuracy
#plt.plot(model_data.history['mean_squared_error'])
#plt.plot(model_data.history['val_mean_squared_error'])
#plt.plot(model_data.history['acc'])
#plt.plot(model_data.history['val_acc'])
#plt.title('model accuracy')
#plt.ylabel('accuracy')
#plt.xlabel('epoch')
#plt.legend(['train', 'test'], loc='upper right')
#plt.savefig('model_accuracy.png',bbox_inches="tight", pad_inches=0, dpi=300)
#plt.show()

# summarize history for loss
plt.plot(model_data.history['loss'])
plt.plot(model_data.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.savefig('model_loss_pkt.png',bbox_inches="tight", pad_inches=0, dpi=300)
plt.show()

# Part 3 - Making the predictions and visualising the results

# Getting the test data set
if web_traffic == True:
    eMBB = True
    if eMBB == True:
        dataset_test = pd.read_csv('/home/mdaa/traffic_prediction/web_traffic/test/traffic_simulation.csv', sep=';')
    else:
        dataset_test = pd.read_csv('/content/drive/My Drive/Colab Notebooks/traffic_prediction/data_URLLC/test_data/traffic_simulation.csv', sep=';')
        
if m2m == True:
#    m2m_model1 = False
    if m2m_model1 == True:
        dataset_test = pd.read_csv('/home/mdaa/traffic_prediction/m2m_traffic/model1_test/traffic_simulation.csv', sep=';')
    else:
        dataset_test = pd.read_csv('/home/mdaa/traffic_prediction/m2m_traffic/model2_test/traffic_simulation.csv', sep=';')
        
if web_traffic == True:
    real_pkt = dataset_test.iloc[web_test_size : web_test_size + result_size, 1:2].values
if m2m == True:
    real_pkt = dataset_test.iloc[m2m_test_size : m2m_test_size + result_size, 1:2].values

# Getting the predicted traffic

inputs = dataset_test.iloc[:, 1:2].values

inputs = inputs.reshape(-1,1)
inputs = sc.transform(inputs)
X_test = []
if web_traffic == True:
    for i in range(web_test_size, web_test_size + result_size):
#       X_test.append(inputs[i-60:i, 0])
        X_test.append(inputs[i-60:i, 0])
        
if m2m == True:
    for i in range(m2m_test_size, m2m_test_size + result_size):
#       X_test.append(inputs[i-60:i, 0])
        X_test.append(inputs[i-30:i, 0])
        
X_test = np.array(X_test)
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))
#X_test = np.reshape(X_test, -1)
predicted_test_pkt = regressor.predict(X_test)
predicted_test_pkt = sc.inverse_transform(predicted_test_pkt)

# Visualising the results

init_val = 0;
prediction_size = 60
x_val = []
for loop in range (prediction_size):
    x_val.append(init_val)
    init_val += 0.1
    

if web_traffic == True:
    plt.plot(real_pkt, color = 'red', label = 'Test Traffic')
    plt.plot(predicted_test_pkt, color = 'blue', label = 'Predicted traffic')
    plt.ylim (900000,965000)
    plt.title('Traffic Prediction')
    plt.xlabel('Time Index (s)')
    #plt.ylabel('Number of Users')
    plt.ylabel('Number of Packets')
    plt.legend(loc='best', prop={'size': 6})
elif m2m == True:
    plt.plot(x_val, real_pkt, color = 'red', label = 'Test Traffic')
    plt.plot(x_val, predicted_test_pkt, color = 'blue', label = 'Predicted traffic')
    plt.ylim (0,5000)
    plt.title('Traffic Prediction')
    plt.xlabel('Time Index (s)')
    plt.ylabel('Number of Packets')
    plt.legend(loc='best', prop={'size': 6})
    

plt.savefig('lstm_testData_pkt.png',bbox_inches="tight", pad_inches=0, dpi=300)
plt.show()

#Generating output for validation dataset
# Creating a data structure with 60 timesteps and 1 output

if web_traffic == True:
    real_valid_data = dataset_train.iloc[web_train_size:web_train_size+result_size, 1:2].values

if m2m == True:
    real_valid_data = dataset_train.iloc[m2m_train_size:m2m_train_size+result_size, 1:2].values
    
X_valid = []


if web_traffic == True:
    for i in range(web_train_size, web_train_size + result_size):
        X_valid.append(training_set_scaled[i-60:i, 0])
        
if m2m == True:
    for i in range(m2m_train_size, m2m_train_size+result_size):
        X_valid.append(training_set_scaled[i-30:i, 0])

X_valid = np.array(X_valid)
X_valid = np.reshape(X_valid, (X_valid.shape[0], X_valid.shape[1], 1))

predicted_packet_valid = regressor.predict(X_valid)
predicted_packet_valid = sc.inverse_transform(predicted_packet_valid)

# Visualising the validation data
#from google.colab import files
#test = plt.figure()

if web_traffic == True:
    plt.plot(real_valid_data, color = 'green', label = 'Validation Traffic')
    plt.plot(predicted_packet_valid, color = 'orange', label = 'Predicted traffic')
    plt.ylim (900000,970000)
    plt.title('Traffic Prediction')
    plt.xlabel('Time Index (s)')
    #plt.ylabel('Number of Users')
    plt.ylabel('Number of Packets')
    plt.legend(loc='best', prop={'size': 6})
elif m2m == True:
    plt.plot(x_val, real_valid_data, color = 'green', label = 'Validation Traffic')
    plt.plot(x_val, predicted_packet_valid, color = 'orange', label = 'Predicted traffic')
    plt.ylim (-1,5000)
    plt.title('Traffic Prediction')
    plt.xlabel('Time Index (s)')
    plt.ylabel('Number of Packets')
    plt.legend(loc='best', prop={'size': 6})

plt.savefig('lstm_valData_pkt.png',bbox_inches="tight", pad_inches=0, dpi=300)
plt.show()

# Visualising the combination
#from google.colab import files
#test = plt.figure()

if web_traffic == True:
    plt.plot(real_pkt, color = 'red', label = 'Test Traffic')
    plt.plot(predicted_test_pkt, color = 'blue', label = 'Predicted traffic')
    plt.plot(real_valid_data, color = 'green', label = 'Validation Traffic')
    plt.plot(predicted_packet_valid, color = 'orange', label = 'Predicted traffic')
    plt.ylim (900000,980000)
    plt.title('Traffic Prediction')
    plt.xlabel('Time Index (s)')
    #plt.ylabel('Number of Users')
    plt.ylabel('Number of Packets')
    plt.legend(loc='best', prop={'size': 6})
elif m2m == True:
    plt.plot(x_val, real_pkt, color = 'red', label = 'Test Traffic')
    plt.plot(x_val, predicted_test_pkt, color = 'blue', label = 'Predicted traffic')
    plt.plot(x_val, real_valid_data, color = 'green', label = 'Validation Traffic')
    plt.plot(x_val, predicted_packet_valid, color = 'orange', label = 'Predicted traffic')
    plt.ylim (-1,5000)
    plt.title('Traffic Prediction')
    plt.xlabel('Time Index (s)')
    plt.ylabel('Number of Packets')
    plt.legend(loc='best', prop={'size': 6})


#plt.draw()
#fig = plt.figure(figsize=(9, 11))
plt.savefig('lstm_comb_pkt.png',bbox_inches="tight", pad_inches=0, dpi=300)
plt.show()
#files.download('test.pdf')

from sklearn.metrics import mean_squared_error

from math import sqrt

mse = mean_squared_error(real_pkt, predicted_test_pkt)

rmse_test = sqrt(mse)

print('RMSE test pkt: %f' % rmse_test)

mse = mean_squared_error(real_valid_data, predicted_packet_valid)

rmse_val = sqrt(mse)

print('RMSE val pkt: %f' % rmse_val)

#Write the test and predicted data
predicted_stock_price = np.reshape (predicted_test_pkt, (np.product(predicted_test_pkt.shape),))

df = pd.DataFrame(list(zip(real_pkt, predicted_test_pkt)),
              columns=['Test Data (number of pkt)','Predicted Data (number of pkt)'])

df.to_csv("lstm_pkt_test.csv", index = None, header=True)

#write the validation and predicted data
predicted_packet_valid = np.reshape (predicted_packet_valid, (np.product(predicted_packet_valid.shape),))

write_val = pd.DataFrame(list(zip(real_valid_data, predicted_packet_valid)),
              columns=['Validation Data (number of pkt)','Predicted Data (number of pkt)'])

write_val.to_csv("lstm_pkt_val.csv", index = None, header=True)